\documentclass{sig-alternate}
\usepackage{natbib}
\bibliographystyle{abbrvnat}

\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{SoICT 2015}{Hue, Vietnam}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

% \title{Ship Image Classification using Deep Convolutional Neural Networks}
\title{A Comparison on Performance of Deep Convolutional Neural Networks: AlexNet and VGG on Image Classification}
% \subtitle{[Extended Abstract]
% \titlenote{A full version of this paper is available as
% \textit{Author's Guide to Preparing ACM SIG Proceedings Using
% \LaTeX$2_\epsilon$\ and BibTeX} at
% \texttt{www.acm.org/eaddress.htm}}}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{2} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Duc-Cuong Dao\\
       \affaddr{School of Information and Communication Technology}\\
       \affaddr{Hanoi University of Science and Technology}\\
       \affaddr{Hanoi, Vietnam}
       \email{duccuong.hust@gmail.com}
% 2nd. author
\alignauthor
Olivier Mor\'{e}re\\
       \affaddr{Agency for Science, Technology and Research (A*STAR)}\\
       % \affaddr{P.O. Box 1212}\\
       \affaddr{Singapore}\\
       \email{olivier.morere@gmail.com}
}
% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.
% \additionalauthors{Additional authors: John Smith (The Th{\o}rv{\"a}ld Group,
% email: {\texttt{jsmith@affiliation.org}}) and Julius P.~Kumquat
% (The Kumquat Consortium, email: {\texttt{jpkumquat@consortium.net}}).}
% \date{30 July 1999}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
We describe the state-of-the-art architecture of Convolutional Neural Networks for image classification tasks.
\end{abstract}

% A category with the (minimum) three required fields
% \category{H.4}{Information Systems Applications}{Miscellaneous}
%A category including the fourth, optional field follows...
% \category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

% \terms{Theory}

\keywords{Deep learning, convolutional neural networks, image classification}

\section{Introduction}

Since their introduction by in the early 1990's, Convolutional Neural Networks (ConvNets) have demonstrated excellent performance at task such as hand-written digit classification and face detection. Recently, serveral papers have shown that they can also deliver outstanding performance on more challenging visual classfication tasks. [Ciresan et al., 2012] demonstrate state-of-the-art performance on NORB and CIFAR-10 datasets. Most notably, [Krizhevsky et al., 2012] show record beating performance on the ImageNet 2012 classification benchmark, with their ConvNet model achieving an error rate of 16.4\%, compared to the 2nd place result of 26.1\%. Several factors are responsible for this renewed interest in ConvNet models: (i) the availibility of much larger training sets, with millions of labeled examples; (ii) powerful GPU implementations, making the traininPOKg of very large models practical and (iii) better model regularization, such as Dropout [Hinton et al., 2012]


\section{Related Work}
Starting with LeNet-5 \citep{NIPS1989_293}, ConvNets have typically had a standard structure - stacked convolutional layers (optionally followed by contrast normalization and max-pooling) are followed by one or more fully-connected layers. Variants of this basic design are prevalent in the image classification literature and have yielded the best results to-date on MNIST, CIFAR and most notably on the ImageNet classification challenge. For larger datasets such as Imagenet, the recent trend has been to increase the number of layer and layer size, while using dropout [7] to address the problem of overfitting.

Despite the concerns that max-pooling layers result in loss of accurate spatial information, the same convolutional network archiecture as [9] has aslo been sucessfully employed for localization, object detection and human post estimation. Inspire by a neuroscience model of the primate visual cortex, .. use a series of fixed Gabor filters of different sizes in order to handle multiple scales, similarly to the Incepttion model. However, contrary to the fixed 2-layer model of [15], all filters in the Inception model are learned. Furthermore, Inception layers are repeated many time, leading to a 22-layer deep model in the case of the GoogLeNet model.

Network-in-Network is an approach proposed by Lin et al. in order to increase the respresentational power of neural newtworks. When appliead to convolutional layers, the method could be viewd as addition $1x1$ convolutional layers follwed typically by the rectified linear activation. This enables it to be easily integrated in the current ConvNet pipelines. We use this approach heavil in our architecture.

\section{Background}
We begin by providing a rough overview of deep learning and its application to various types of problem in machine learning and describe the architecture of typical ConvNet. After that, we describe the traditional learning strategies of ConvNets for a particular problem. Although ConvNets can be trained for unspervised learning tasks, we concentrate on the supervised learning as our target problem (image classification) falls into that family (Need to paraphase)
\subsection{Deep Learning}
Deep Learning is a branch of machine learning that allows computational models to learn complicated and abstract representation of features.
\subsection{Convolutional Neural Networks}
ConvNets are hierarchical neural networks whose convolutional layers alternate with subsampling layers, reminiscent of simple and complex ceels in the primary visual cortex. ConvNets vary in how convolutional and subsample layer are realized and how they are trained.
       \subsubsection{Convolutional Layer}
       A convolutional layer is parametrized by the size and the number of maps, kernel sizes, skipping factors and the connection table. Each layer has M maps of equal size ($M_x, M_y$). A kernel of size ($K_x, K_y$) is shifted over the valid region of the input image (i.e. the kernel has be completely inside the image). The skipping factors $S_x$ and $S_y$ define how many pixels the filter/kernel skips in x- and y-direction between subsequent convolutions. The size of the output maps is then defined as:
       \begin{equation}
              M^n_x=\frac{M^{n-1}_x - K^n_x}{S^n_x + 1}+1; M^n_y=\frac{M^{n-1}_y - K^n_y}{S^n_y + 1}+1
       \end{equation}
       where index $n$ indicates the layer. Each map in layer $L^n$ is connected to at most $M^{n-1}$. Neurons of a given map share their weights but have different receptive fields.
       \subsubsection{Pooling Layer}
       The biggest architectural difference between our implementation and the ConvNet of [leCun et al] is the use of a max-pooling layer instead of a sub-smapling layer. No such layer is used by [Simard et al..] who simply skips nearby pixels prior to convolution, instead of pooling or averaging. 
       \subsubsection{Classification Layer}
\section{Experiments}
\subsection{The Dataset}
We used a crawler to collect ship images from different sources along with their meta information (name, category, etc.).
\subsection{Training CNNs for Image Classification and Deep Features}
\subsection{Experimental Results}
\section{Conclusions}

%\end{document}  % This is where a 'short' article might terminate

%ACKNOWLEDGMENTS are optional
\section{Acknowledgments}
We would like to thank Assoc. Prof., St\'{e}phane Bressan at School of Computing, National University of Singapore for insightful supervision during the work of this paper and to Dr. Antoine Veillard at Image Persuasive Lab (IPAL), Insitute of Infocomm Research (I2R), A*STAR for valuable discussions. 

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{dlbib}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns
% \appendix
% %Appendix A
% \section{Headings in Appendices}
% The rules about hierarchical headings discussed above for
% the body of the article are different in the appendices.
% In the \textbf{appendix} environment, the command
% \textbf{section} is used to
% indicate the start of each Appendix, with alphabetic order
% designation (i.e. the first is A, the second B, etc.) and
% a title (if you include one).  So, if you need
% hierarchical structure
% \textit{within} an Appendix, start with \textbf{subsection} as the
% highest level. Here is an outline of the body of this
% document in Appendix-appropriate form:
% \subsection{Introduction}
% \subsection{The Body of the Paper}
% \subsubsection{Type Changes and  Special Characters}
% \subsubsection{Math Equations}
% \paragraph{Inline (In-text) Equations}
% \paragraph{Display Equations}
% \subsubsection{Citations}
% \subsubsection{Tables}
% \subsubsection{Figures}
% \subsubsection{Theorem-like Constructs}
% \subsubsection*{A Caveat for the \TeX\ Expert}
% \subsection{Conclusions}
% \subsection{Acknowledgments}
% \subsection{Additional Authors}
% This section is inserted by \LaTeX; you do not insert it.
% You just add the names and information in the
% \texttt{{\char'134}additionalauthors} command at the start
% of the document.
% \subsection{References}
% Generated by bibtex from your ~.bib file.  Run latex,
% then bibtex, then latex twice (to resolve references)
% to create the ~.bbl file.  Insert that ~.bbl file into
% the .tex source file and comment out
% the command \texttt{{\char'134}thebibliography}.
% % This next section command marks the start of
% % Appendix B, and does not continue the present hierarchy
% \section{More Help for the Hardy}
% The sig-alternate.cls file itself is chock-full of succinct
% and helpful comments.  If you consider yourself a moderately
% experienced to expert user of \LaTeX, you may find reading
% it useful but please remember not to change it.
%\balancecolumns % GM June 2007
% That's all folks!
\end{document}
